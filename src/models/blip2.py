"""BLIP-2 caption generation wrapper for video frames."""
from __future__ import annotations

import logging
from typing import Iterable, List

import numpy as np
import torch
from PIL import Image

from .base import BaseEncoder

LOGGER = logging.getLogger(__name__)

# Defer imports to avoid dependency issues
Blip2Processor = None
Blip2ForConditionalGeneration = None


def _try_import_blip2():
    """Try to import BLIP-2 components at runtime."""
    global Blip2Processor, Blip2ForConditionalGeneration
    if Blip2Processor is not None:
        return True
    
    try:
        from transformers import (
            Blip2Processor as _Blip2Processor,
            Blip2ForConditionalGeneration as _Blip2ForConditionalGeneration
        )
        Blip2Processor = _Blip2Processor
        Blip2ForConditionalGeneration = _Blip2ForConditionalGeneration
        return True
    except Exception as e:
        LOGGER.warning(f"Could not import BLIP-2: {e}")
        return False


class BLIP2Captioner(BaseEncoder):
    """Generates natural language captions for video frames using BLIP-2."""
    
    def __init__(
        self, 
        model_name: str = "Salesforce/blip2-opt-2.7b",
        max_length: int = 50,
        **kwargs
    ):
        """
        Initialize BLIP-2 captioner.
        
        Args:
            model_name: BLIP-2 model variant
            max_length: Maximum caption length in tokens
        """
        super().__init__(**kwargs)
        self.model_name = model_name
        self.max_length = max_length
        self.model = None
        self.processor = None
        self._use_fallback = False
    
    def load(self) -> None:
        """Load the BLIP-2 model and processor."""
        if not _try_import_blip2():
            LOGGER.warning("BLIP-2 not available, using fallback captioner")
            self._use_fallback = True
            return
        
        try:
            import os
            
            # Get HF token from environment
            token = os.environ.get("HF_TOKEN") or os.environ.get("HUGGING_FACE_HUB_TOKEN")
            
            # Use smaller, faster BLIP model for better scene descriptions
            model_name = "Salesforce/blip-image-captioning-base"  # Faster and better than BLIP-2 for simple captions
            LOGGER.info(f"Loading BLIP model: {model_name}")
            
            # Try cache first
            try:
                from transformers import BlipProcessor, BlipForConditionalGeneration
                self.processor = BlipProcessor.from_pretrained(model_name, token=token, local_files_only=True)
                self.model = BlipForConditionalGeneration.from_pretrained(
                    model_name,
                    token=token,
                    local_files_only=True,
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
                )
                LOGGER.info("âœ… Loaded BLIP from cache (offline mode)")
            except:
                # Download if not in cache
                from transformers import BlipProcessor, BlipForConditionalGeneration
                self.processor = BlipProcessor.from_pretrained(model_name, token=token)
                self.model = BlipForConditionalGeneration.from_pretrained(
                    model_name,
                    token=token,
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32
                )
            
            self.model.to(self.device)
            self.model.eval()
            
            LOGGER.info("BLIP model loaded successfully")
            self._use_fallback = False
        except Exception as e:
            LOGGER.warning(f"BLIP model unavailable, using fallback: {e}")
            self.model = None
            self.processor = None
            self._use_fallback = True
    
    @torch.no_grad()
    def generate_captions(self, frames: Iterable[np.ndarray]) -> dict:
        """
        Generate captions for video frames.
        
        Args:
            frames: List of frames as numpy arrays (H, W, C)
        
        Returns:
            Dictionary with generated captions and metadata
        """
        frames_list = list(frames)
        
        # Check if models are loaded before proceeding
        if self._use_fallback or self.model is None or self.processor is None:
            return self._fallback_caption(frames_list)
        
        # Strip alpha channel if present
        frames_list = [
            frame[:, :, :3] if frame.shape[2] == 4 else frame
            for frame in frames_list
        ]
        
        try:
            captions = []
            
            # Sample frames to avoid processing too many
            sample_indices = np.linspace(
                0, len(frames_list) - 1, min(5, len(frames_list)), dtype=int
            )
            sampled_frames = [frames_list[i] for i in sample_indices]
            
            for frame in sampled_frames:
                # Convert to PIL Image (RGB)
                pil_image = Image.fromarray(frame.astype(np.uint8))
                
                # Generate caption
                inputs = self.processor(images=pil_image, return_tensors="pt").to(
                    self.device, 
                    torch.float16 if self.device.type == "cuda" else torch.float32
                )
                
                generated_ids = self.model.generate(
                    **inputs,
                    max_length=self.max_length,
                    num_beams=5,  # More beams for better quality
                    min_length=10,  # Ensure detailed captions
                    length_penalty=1.0,
                    early_stopping=True
                )
                
                caption = self.processor.batch_decode(
                    generated_ids, skip_special_tokens=True
                )[0].strip()
                
                captions.append(caption)
            
            # Combine captions - use the longest or most common
            if captions:
                # Use the longest caption as primary
                primary_caption = max(captions, key=len)
                
                return {
                    "caption": primary_caption,
                    "all_captions": captions,
                    "num_frames_captioned": len(sampled_frames),
                    "confidence": 0.85  # BLIP-2 typically has good confidence
                }
            else:
                return self._fallback_caption(frames_list)
        
        except Exception as e:
            LOGGER.error(f"BLIP-2 caption generation failed: {e}")
            return self._fallback_caption(frames_list)
    
    def _fallback_caption(self, frames: List[np.ndarray]) -> dict:
        """Fallback caption generation using visual analysis."""
        LOGGER.debug("Using fallback caption generation")
        
        if not frames:
            return {
                "caption": "A video scene",
                "all_captions": ["A video scene"],
                "num_frames_captioned": 0,
                "confidence": 0.1,
                "fallback": True
            }
        
        # Analyze scene characteristics
        avg_brightness = np.mean([frame.mean() for frame in frames])
        avg_saturation = np.mean([np.std(frame) for frame in frames])
        
        # Detect dominant colors
        avg_frame = np.mean(frames, axis=0).astype(np.uint8)
        r, g, b = avg_frame[:,:,0].mean(), avg_frame[:,:,1].mean(), avg_frame[:,:,2].mean()
        
        # Build descriptive caption
        parts = []
        
        # Lighting
        if avg_brightness > 180:
            parts.append("a bright")
        elif avg_brightness > 120:
            parts.append("a well-lit")
        elif avg_brightness > 60:
            parts.append("a dimly lit")
        else:
            parts.append("a dark")
        
        # Setting (based on color analysis)
        if g > r and g > b:
            parts.append("outdoor scene with greenery")
        elif b > r and b > g and avg_brightness > 100:
            parts.append("outdoor scene with sky")
        elif avg_saturation < 30:
            parts.append("indoor scene")
        else:
            parts.append("scene with various objects")
        
        # Motion (based on frame differences)
        if len(frames) > 1:
            frame_diff = np.mean([np.abs(frames[i] - frames[i-1]).mean() 
                                 for i in range(1, min(5, len(frames)))])
            if frame_diff > 20:
                parts.append("with movement")
            elif frame_diff > 5:
                parts.append("with slight motion")
        
        caption = " ".join(parts).capitalize()
        
        return {
            "caption": caption,
            "all_captions": [caption],
            "num_frames_captioned": len(frames),
            "confidence": 0.4,
            "fallback": True
        }
    
    def encode(self, frames: Iterable[np.ndarray]) -> np.ndarray:
        """Encode frames (required by BaseEncoder interface)."""
        # For BLIP-2, we return a text-based embedding
        caption_result = self.generate_captions(frames)
        caption = caption_result.get("caption", "")
        
        # Simple text embedding using character encoding
        embedding = np.zeros(512, dtype=np.float32)
        for i, char in enumerate(caption[:512]):
            embedding[i] = ord(char) / 255.0
        
        # Normalize
        norm = np.linalg.norm(embedding) + 1e-8
        return (embedding / norm).reshape(1, -1)


__all__ = ["BLIP2Captioner"]
